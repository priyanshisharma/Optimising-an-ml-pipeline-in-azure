# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
I was working with the Bank-Marketing dataset, that contains information about a previous bank marketing campaign, i.e.
attributed of the people the campaign dealt with and how was the outcome (binary outcome) for the same. <br/>
We seek to predict that outcome. This can be helpful in developing future marketing campaigns.

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
I received 91.514930% accuracy with hyperdrive and logistic regression, and the best model that AutoML fetched was the 
VotingEnsemble with an accuracy of 91.976%, making VotingEnsemble the best performing model. 

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

* I start by defining *training script*, for the estimator required by Hyperdrive. This is done in a separate folder called `train.py`, and uses **Logistic Regression** from Scikit-learn.
* This was followed by defining the *sampling method over the search space* using **RandomParameterSampling**.
* I then defined my *early termination policy* using **BanditPolicy**.
* Keeping *Accuracy* as my primary metric I increased the max_total_runs to 20 and defined my **HyperDriveConfig**.
* I then run Hyperdrive.

**What are the benefits of the parameter sampler you chose?**
Random sampling's biggest benefit is elimination of bias, i.e. crucial in critical sectors like Banking.

**What are the benefits of the early stopping policy you chose?**
Bandit terminates runs where the primary metric is not within the specified slack factor/slack amount compared to the best performing run.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
* Model - The Voting Ensemble
* Hyperparameters: The models ensembled appear to be arranged as follows:
    * "iteration": "28",
    * "ensembled_iterations": "[1, 0, 13, 3, 9, 5]",
    * "ensembled_algorithms": "['XGBoostClassifier', 'LightGBM', 'SGD', 'SGD', 'SGD', 'SGD']",
    * "ensemble_weights": "[0.4666666666666667, 0.26666666666666666, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667, 0.06666666666666667]",
        

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
* There isn't a huge difference in the accuracy, but automl definitely has an edge which might extrapolate with a bigger dataset.
* There is significant difference in the architecture of the models, that is due to the range of possibilities and variety AutoML had. Needless to mention, these may also not be as intuitive while starting, hence AutoML can give valuable insights to get on the right track.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
Different models instead of Logistic Regression maybe used and coompared. The models to be chosen can also be derived from the AutoML run itself.
